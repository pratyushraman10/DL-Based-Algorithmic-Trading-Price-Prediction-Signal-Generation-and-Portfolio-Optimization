# -*- coding: utf-8 -*-
"""algtrad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12aQNz8DH8Wkf3OQqBz4qy1ctRL1UuRzq
"""

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import math
import random
import matplotlib.pyplot as plt
import yfinance as yf

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Config (tweakable)
CFG = {
    "ticker": "AAPL",
    "start": "2015-01-01",
    "end": "2024-12-31",
    "sequence_length": 60,
    "prediction_horizon": 1,       # predict 1 period ahead
    "model_type": "hybrid",        # "lstm", "transformer", "hybrid"
    "target_mode": "regression",   # "regression" or "classification"
    "classification_threshold": 0.0,
    "batch_size": 128,
    "epochs": 40,
    "lr": 1e-4,
    "hidden_size": 128,
    "num_layers": 2,
    "dropout": 0.15,
    "seed": 42,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    # Backtest params
    "initial_capital": 100000.0,
    "transaction_cost": 0.0005,  # fraction per trade (0.05%)
    "slippage": 0.0005,
    "risk_fraction": 0.01,       # fraction of capital risked per trade (vol scaling)
    "max_position_size": 0.25,
    "use_sharpe_loss": False     # set True to include sharpe-loss in training (experimental)
}

# Reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(CFG["seed"])

# Data loading (replaceable)
def load_price_data(ticker, start, end):
    import yfinance as yf
    import pandas as pd
    import numpy as np

    df = yf.download(ticker, start=start, end=end, progress=False, group_by='ticker')

    if df.empty:
        raise ValueError(f"No data downloaded for {ticker} between {start} and {end}")

    # Flatten MultiIndex columns if needed
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in df.columns]
        df.columns = [c.replace(f"{ticker} ", "") for c in df.columns]

    # Ensure "Adj Close" exists, else copy from "Close"
    if "Adj Close" not in df.columns and "Close" in df.columns:
        df["Adj Close"] = df["Close"]

    expected_cols = {"Open", "High", "Low", "Close", "Adj Close", "Volume"}
    missing = expected_cols - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns: {missing}. Got: {df.columns.tolist()}")

    # Ensure Close is float
    df["Close"] = pd.to_numeric(
        df["Close"].apply(lambda x: np.array(x).item() if isinstance(x, (np.ndarray, list)) else x),
        errors="coerce"
    )
    df.dropna(subset=["Close"], inplace=True)

    return df

# Feature engineering (robust)
def add_technical_indicators(df):
    """
    Adds features: returns, log returns, EMA(10), EMA(50), SMA(20), RSI(14), rolling vol (21), ATR(14)
    Implementation is defensive to shape issues. Returns df with new columns.
    """
    df = df.copy()
    # Ensure Close, High, Low are 1D numeric
    for col in ['Close', 'High', 'Low', 'Open', 'Volume']:
        if col in df.columns:
            df[col] = pd.to_numeric(pd.Series(np.ravel(df[col].values), index=df.index), errors='coerce')
    # Basic returns
    df['ret'] = df['Close'].pct_change()
    df['logret'] = np.log1p(df['ret'].fillna(0.0))
    # EMA and SMA (simple implementations to avoid external hazards)
    df['ema_10'] = df['Close'].ewm(span=10, adjust=False).mean()
    df['ema_50'] = df['Close'].ewm(span=50, adjust=False).mean()
    df['sma_20'] = df['Close'].rolling(window=20, min_periods=1).mean()
    # RSI implementation
    delta = df['Close'].diff()
    up = delta.clip(lower=0)
    down = -1 * delta.clip(upper=0)
    roll_up = up.rolling(14, min_periods=1).mean()
    roll_down = down.rolling(14, min_periods=1).mean()
    rs = roll_up / (roll_down + 1e-9)
    df['rsi_14'] = 100 - (100 / (1 + rs))
    df['rsi_14'].fillna(50, inplace=True)
    # ATR approx
    tr = pd.concat([
        (df['High'] - df['Low']).abs(),
        (df['High'] - df['Close'].shift(1)).abs(),
        (df['Low'] - df['Close'].shift(1)).abs()
    ], axis=1).max(axis=1)
    df['atr_14'] = tr.rolling(window=14, min_periods=1).mean()
    # rolling volatility
    df['vol_21'] = df['ret'].rolling(21, min_periods=1).std().fillna(0.0)
    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)
    return df

# Data validation & scaling
def prepare_features(df, feature_cols):
    """
    Validates dataset (removes non-numeric rows), checks length and scales features.
    Returns: features_df (aligned), scaler object
    """
    df = df.copy()
    # Coerce feature columns to numeric, drop rows with NA in those columns
    for col in feature_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    df.dropna(subset=feature_cols, inplace=True)
    if df.shape[0] == 0:
        raise ValueError("No data left after cleaning features. Check input data and indicators.")
    scaler = StandardScaler()
    scaled = scaler.fit_transform(df[feature_cols].values)
    df_scaled = pd.DataFrame(scaled, index=df.index, columns=feature_cols)
    # Keep Close original (we will need for returns/backtest). If you want scaled Close in features, include it in feature_cols.
    if 'Close' not in df_scaled.columns and 'Close' in df.columns:
        df_scaled['Close'] = df['Close']
    return df_scaled, scaler

# Sequence creation (robust)
def create_sequences(df_scaled, raw_close_series, feature_cols, seq_len, pred_horizon, cfg):
    """
    df_scaled: DataFrame with numeric feature columns (index aligned)
    raw_close_series: original Close Series (1D numeric) aligned with df_scaled index
    Returns: X (N, seq_len, d), y (N,1), start_indices (list of original df index positions for each sample)
    """
    arr = df_scaled[feature_cols].values.astype(np.float32)
    closes = pd.to_numeric(pd.Series(np.ravel(raw_close_series.values), index=raw_close_series.index), errors='coerce').values.astype(np.float32).reshape(-1)
    if len(arr) != len(closes):
        raise ValueError("Feature array and close array length mismatch.")
    n = len(arr)
    X_list = []
    y_list = []
    idx_list = []
    # We will generate windows where the last index of the window is i-1 and we predict i+pred_horizon-1
    for end in range(seq_len, n - pred_horizon + 1):
        start = end - seq_len
        x = arr[start:end]                     # shape (seq_len, d)
        future_close = closes[end + pred_horizon - 1]
        past_close = closes[end - 1]
        # Guard against impossible values
        if np.isnan(future_close) or np.isnan(past_close) or past_close == 0:
            continue
        future_return = float(future_close / past_close - 1.0)
        if cfg["target_mode"] == "classification":
            label = 1.0 if future_return > cfg["classification_threshold"] else 0.0
            y_list.append([label])
        else:
            y_list.append([future_return])
        X_list.append(x)
        idx_list.append(end)  # sample corresponds to original index position 'end'
    if len(X_list) == 0:
        raise ValueError("No sequences createdâ€”dataset too small or too many NaNs.")
    X = np.stack(X_list)
    y = np.array(y_list, dtype=np.float32)
    return X, y, idx_list

# Dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).float()
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Models: LSTM, Transformer, Hybrid
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.1, out_dim=1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, out_dim)
        )
    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        return self.fc(last)

class TransformerModel(nn.Module):
    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1, out_dim=1):
        super().__init__()
        self.proj = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, out_dim)
        )
    def forward(self, x):
        x = self.proj(x)
        x = self.transformer(x)
        x = x.permute(0, 2, 1)
        x = self.pool(x).squeeze(-1)
        return self.fc(x)

class HybridModel(nn.Module):
    def __init__(self, input_dim, hidden_size=128, lstm_layers=1, transformer_heads=4, transformer_layers=1, dropout=0.1, out_dim=1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers=lstm_layers, batch_first=True, dropout=dropout)
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=transformer_heads, dim_feedforward=hidden_size*2, dropout=dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, out_dim)
        )
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.transformer(out)
        last = out[:, -1, :]
        return self.fc(last)

# Losses: MSE + Sharpe loss
def sharpe_loss(preds, rets):
    """
    preds: (batch,) predicted positions (continuous), rets: (batch,) actual returns
    Returns negative Sharpe to minimize.
    """
    strat_ret = preds.squeeze() * rets.squeeze()
    mean = torch.mean(strat_ret)
    std = torch.std(strat_ret) + 1e-9
    neg_sharpe = - (mean / std)
    return neg_sharpe

# Training / Evaluation
def train(model, train_loader, val_loader, cfg):
    device = cfg["device"]
    model = model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=cfg["lr"], weight_decay=1e-6)
    mse = nn.MSELoss()
    best_val = float('inf')
    best_state = None
    patience = 8
    patience_ctr = 0

    for epoch in range(cfg["epochs"]):
        model.train()
        train_losses = []
        for Xb, yb in train_loader:
            Xb = Xb.to(device)
            yb = yb.to(device)
            opt.zero_grad()
            out = model(Xb).squeeze(-1)
            if cfg["target_mode"] == "classification":
                # regression-style raw output; apply BCE with sigmoid if you want prob outputs
                loss = nn.BCEWithLogitsLoss()(out, yb.squeeze(-1))
            else:
                loss = mse(out, yb.squeeze(-1))
            # optional sharpe component (helps with direct objective sometimes)
            if cfg.get("use_sharpe_loss", False):
                # normalize returns for sharpe loss stability (use yb as returns)
                loss = loss + 0.01 * sharpe_loss(out, yb.squeeze(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
            train_losses.append(loss.item())
        val_loss = evaluate_loss(model, val_loader, cfg)
        # early stopping
        if val_loss < best_val - 1e-6:
            best_val = val_loss
            best_state = {k: v.cpu() for k, v in model.state_dict().items()}
            patience_ctr = 0
        else:
            patience_ctr += 1
            if patience_ctr >= patience:
                break
        # simple progress
        print(f"Epoch {epoch+1}/{cfg['epochs']} - train_loss: {np.mean(train_losses):.6f} val_loss: {val_loss:.6f}")
    if best_state is not None:
        model.load_state_dict(best_state)
    return model

def evaluate_loss(model, loader, cfg):
    device = cfg["device"]
    model.eval()
    losses = []
    mse = nn.MSELoss()
    with torch.no_grad():
        for Xb, yb in loader:
            Xb = Xb.to(device)
            yb = yb.to(device)
            out = model(Xb).squeeze(-1)
            if cfg["target_mode"] == "classification":
                loss = nn.BCEWithLogitsLoss()(out, yb.squeeze(-1))
            else:
                loss = mse(out, yb.squeeze(-1))
            losses.append(loss.item())
    return np.mean(losses) if len(losses) > 0 else float('inf')

def predict(model, loader, cfg, apply_sigmoid_for_prob=False):
    device = cfg["device"]
    model.eval()
    preds = []
    trues = []
    with torch.no_grad():
        for Xb, yb in loader:
            Xb = Xb.to(device)
            out = model(Xb).squeeze(-1).detach().cpu().numpy()
            preds.append(out)
            trues.append(yb.numpy().squeeze(-1))
    preds = np.concatenate(preds, axis=0)
    trues = np.concatenate(trues, axis=0)
    if apply_sigmoid_for_prob:
        preds = 1 / (1 + np.exp(-preds))  # sigmoid
    return preds, trues

# Backtest function
def backtest(df_indexed, sample_indices, preds, cfg):
    """
    df_indexed: original df with Close prices indexed (same index used to create sequences)
    sample_indices: list of integer positions (end index positions used when creating sequences)
    preds: model predictions aligned to sample_indices (length n_samples,)
    """
    # align into a DataFrame for simulation
    out = pd.DataFrame({
        "pos_idx": sample_indices,
        "pred": preds
    })
    # map pos_idx to actual dates and close prices
    out['date'] = df_indexed.index[out['pos_idx']]
    out['close'] = df_indexed['Close'].values[out['pos_idx']]
    # compute signals
    if cfg["target_mode"] == "classification":
        # pred is probability if model used sigmoid; else threshold raw
        out['signal'] = (out['pred'] > 0.5).astype(int)
    else:
        # regression: derive signal by sign and optionally magnitude threshold
        out['signal'] = np.where(out['pred'] > 0, 1, 0)

    capital = cfg["initial_capital"]
    cash = capital
    holdings = 0.0
    equity_curve = []
    last_position = 0

    for i, row in out.iterrows():
        price = float(row['close'])
        signal = int(row['signal'])
        atr = float(df_indexed['atr_14'].values[row['pos_idx']]) if 'atr_14' in df_indexed.columns else 0.0
        atr = max(atr, 1e-6)
        # position sizing by volatility (approx)
        dollar_risk = cfg['risk_fraction'] * capital
        units = (dollar_risk / (atr * price)) if (atr * price) > 0 else 0
        notional = units * price
        max_notional = cfg['max_position_size'] * capital
        if notional > max_notional:
            notional = max_notional
            units = notional / price

        # trading logic: simple long-only
        if last_position == 0 and signal == 1:
            # open position
            trade_cost = price * units * (cfg['transaction_cost'] + cfg['slippage'])
            holdings = units
            cash -= units * price + trade_cost
            last_position = 1
        elif last_position == 1 and signal == 0:
            # close
            trade_cost = price * holdings * (cfg['transaction_cost'] + cfg['slippage'])
            cash += holdings * price - trade_cost
            holdings = 0
            last_position = 0
        # compute equity
        equity = cash + holdings * price
        equity_curve.append(equity)

    # metrics
    res = pd.DataFrame({
        "date": out['date'].values[:len(equity_curve)],
        "equity": equity_curve
    })
    res['returns'] = res['equity'].pct_change().fillna(0)
    total_return = res['equity'].iloc[-1] / res['equity'].iloc[0] - 1.0
    ann_return = (res['equity'].iloc[-1] / res['equity'].iloc[0]) ** (252.0 / max(len(res),1)) - 1.0
    sharpe = (res['returns'].mean() / (res['returns'].std() + 1e-9)) * math.sqrt(252) if res['returns'].std() > 0 else 0.0
    max_dd = max_drawdown(res['equity'].values) if len(res) > 0 else 0.0
    return res, {"total_return": total_return, "annual_return": ann_return, "sharpe": sharpe, "max_drawdown": max_dd}

def max_drawdown(arr):
    peak = arr[0]
    max_dd = 0.0
    for v in arr:
        if v > peak:
            peak = v
        dd = (peak - v) / (peak + 1e-9)
        if dd > max_dd:
            max_dd = dd
    return max_dd

# End-to-end run
def run(cfg):
    print("Device:", cfg["device"])
    # 1) Load
    df_raw = load_price_data(cfg["ticker"], cfg["start"], cfg["end"])
    print("Loaded rows:", len(df_raw))
    # 2) Features
    df_feat = add_technical_indicators(df_raw)
    feature_cols = ['ret', 'logret', 'ema_10', 'ema_50', 'sma_20', 'rsi_14', 'atr_14', 'vol_21']
    # 3) Prepare & scale
    df_scaled, scaler = prepare_features(df_feat, feature_cols)
    # keep original close series aligned to df_scaled (reindex)
    df_aligned = df_feat.loc[df_scaled.index].copy()
    # 4) Create sequences
    X, y, idx_list = create_sequences(df_scaled, df_aligned['Close'], feature_cols, cfg["sequence_length"], cfg["prediction_horizon"], cfg)
    print("Created sequences:", X.shape, y.shape)
    # 5) Train/Val/Test split (time-series)
    n = len(X)
    train_end = int(0.7 * n)
    val_end = int(0.85 * n)
    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]
    idx_test = idx_list[val_end:]

    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=cfg["batch_size"], shuffle=True)
    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=cfg["batch_size"], shuffle=False)
    test_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=cfg["batch_size"], shuffle=False)

    # 6) Build model
    input_dim = X.shape[2]
    if cfg["model_type"] == "lstm":
        model = LSTMModel(input_dim, hidden_size=cfg["hidden_size"], num_layers=cfg["num_layers"], dropout=cfg["dropout"], out_dim=1)
    elif cfg["model_type"] == "transformer":
        model = TransformerModel(input_dim, d_model=cfg["hidden_size"], nhead=4, num_layers=cfg["num_layers"], dropout=cfg["dropout"], out_dim=1)
    else:
        # hybrid
        model = HybridModel(input_dim, hidden_size=cfg["hidden_size"], lstm_layers=cfg["num_layers"], transformer_heads=4, transformer_layers=cfg["num_layers"], dropout=cfg["dropout"], out_dim=1)

    # 7) Train
    model = train(model, train_loader, val_loader, cfg)

    # 8) Predict on test set
    preds, trues = predict(model, test_loader, cfg, apply_sigmoid_for_prob=False)
    # If classification and model output is logits, convert by sigmoid
    if cfg["target_mode"] == "classification":
        preds_prob = 1/(1+np.exp(-preds))
        preds_used = preds_prob
    else:
        preds_used = preds  # regression predicted returns

    # 9) Backtest
    res_df, metrics = backtest(df_aligned, idx_test, preds_used, cfg)
    print("Backtest metrics:", metrics)

    # 10) Plot equity
    if len(res_df) > 0:
        plt.figure(figsize=(10,4))
        plt.plot(res_df['date'], res_df['equity'])
        plt.title(f"Equity curve ({cfg['model_type']})")
        plt.xlabel("Date")
        plt.ylabel("Equity")
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    else:
        print("No backtest rows to plot (empty).")

    return model, res_df, metrics

# Run script
if __name__ == "__main__":
    model, results, metrics = run(CFG)
    print("Done.")

# Dummy plots to predict behaviour

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

sns.set(style="whitegrid", context="talk")


# Dummy Data for Testing
dates = pd.date_range(start='2025-01-01', periods=50)
actual_price = np.linspace(100, 150, 50) + np.random.randn(50)*2
predicted_price = actual_price + np.random.randn(50)*3

results = pd.DataFrame({
    'Actual': actual_price,
    'Predicted': predicted_price
}, index=dates)

signals = pd.DataFrame({
    'Signal': [1, -1, 1, -1, 1],
    'Price': [105, 110, 115, 120, 125]
}, index=dates[[5,10,15,20,25]])

portfolio_values = pd.Series(np.cumsum(np.random.randn(50)*2 + 1) + 100, index=dates)


# 1. Price + Predicted + Signals
plt.figure(figsize=(16,6))
sns.lineplot(x=results.index, y=results['Actual'], label="Actual Price", color='blue')
sns.lineplot(x=results.index, y=results['Predicted'], label="Predicted Price", color='orange')
sns.scatterplot(x=signals.index, y=signals['Price'], hue=signals['Signal'].map({1:'Buy', -1:'Sell'}),
                palette={'Buy':'green','Sell':'red'}, s=100)
plt.title("Price vs Predicted Price + Signals")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.show()


# 2. Portfolio / Cumulative Returns
plt.figure(figsize=(16,6))
portfolio_norm = portfolio_values / portfolio_values.iloc[0]
sns.lineplot(x=portfolio_norm.index, y=portfolio_norm, color='orange', label='Portfolio (normalized)')
plt.title("Portfolio Growth (Normalized)")
plt.xlabel("Date")
plt.ylabel("Normalized Portfolio Value")
plt.legend()
plt.show()


# 3. Daily Returns
daily_returns = portfolio_values.pct_change().fillna(0)
plt.figure(figsize=(16,4))
sns.barplot(x=daily_returns.index, y=daily_returns.values, color='grey')
plt.title("Daily Returns")
plt.xlabel("Date")
plt.ylabel("Daily Return")
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

sns.set(style="whitegrid", context="talk")

# 1. Price + Predicted + Signals
plt.figure(figsize=(16,6))

# Plot actual price
if 'results' in locals() and not results.empty:
    actual_col = next((c for c in results.columns if 'close' in c.lower() or 'actual' in c.lower()), None)
    predicted_col = next((c for c in results.columns if 'pred' in c.lower() or 'y_hat' in c.lower()), None)

    if actual_col:
        sns.lineplot(x=results.index, y=results[actual_col], label="Actual Price", color='blue')
    if predicted_col:
        sns.scatterplot(x=results.index, y=results[predicted_col], label="Predicted Price", color='orange', s=40)

# Overlay buy/sell signals
if 'signals' in locals() and not signals.empty:
    buy = signals[signals['Signal']==1]
    sell = signals[signals['Signal']==-1]
    if not buy.empty:
        sns.scatterplot(x=buy.index, y=buy['Price'], label='Buy Signal', marker='^', color='green', s=100)
    if not sell.empty:
        sns.scatterplot(x=sell.index, y=sell['Price'], label='Sell Signal', marker='v', color='red', s=100)

plt.title("Price vs Predicted Price + Signals")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.tight_layout()
plt.show()

# 2. Portfolio / Cumulative Returns
if 'portfolio_values' in locals() and not portfolio_values.empty:
    plt.figure(figsize=(16,6))
    portfolio_norm = portfolio_values / portfolio_values.iloc[0]  # normalize
    sns.lineplot(x=portfolio_norm.index, y=portfolio_norm, color='orange', label='Portfolio (normalized)')
    plt.title("Portfolio Growth (Normalized)")
    plt.xlabel("Date")
    plt.ylabel("Normalized Portfolio Value")
    plt.legend()
    plt.tight_layout()
    plt.show()

# 3. Daily Returns
if 'portfolio_values' in locals() and not portfolio_values.empty:
    daily_returns = portfolio_values.pct_change().fillna(0)
    plt.figure(figsize=(16,4))
    sns.barplot(x=daily_returns.index, y=daily_returns.values, color='grey')
    plt.title("Daily Returns")
    plt.xlabel("Date")
    plt.ylabel("Daily Return")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

